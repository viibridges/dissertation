\chapter{Introduction}
\label{chap:intro}

\makeatletter
\newenvironment{chapquote}[2][2em]
{\setlength{\@tempdima}{#1} \def\chapquote@author{#2} \parshape 1
  \@tempdima \dimexpr\textwidth-2\@tempdima\relax \itshape}
{\par\normalfont\hfill--\
\chapquote@author\hspace*{\@tempdima}\par\bigskip}
\makeatother

%\begin{chapquote}{Walt Disney}
  %``Of all of our inventions for mass communication, pictures still
  %speak the most universally understood language.''
%\end{chapquote}

%Outline:
\begin{itemize}
  \item sliver bullet and ``summer project'' story
  \item \st{Geometry is important.}
  \item \st{traditional method for geometry detection}
  \item \st{deep learning (gamer changer)}
  \item \st{geometry detection with deep learning}
  \item geometry for deep learning training
  \item overhead images
\end{itemize}

\brief{
The topic of understanding images has been around for half a century.
A well known [annodose] in computer vision community is set in the
60s. The MIT professor Dr. Marvin Minsky assigned a couple of
undergrads to spend the summer programming a computer to recognize
objects in the scene~\cite{boden2006mind}. Quite like the attempts of
proving the famous ``Four color theorem'', [[story about a prof claim
he can prove it in a class]]. Fortunate for the mathematicans, the
``Four color theorem'' has been proved by using powerful computational
resources, while the computer scientists are still trapped in Dr.
Marvin Minsky's summer program (not fully solved yet).
}

It is not a trivial task for machines to understand images in general.
To achieve this goal, researchers adopt a divide-and-conquer strategy,
that is divide image understanding into various sub-problems like
object recognition, semantic segmentation, and pose estimation \etc.
During the past decades, a lot of methods are developed to solve there
sub-problem, many of them adopted a bottom-up (\todo{citations})
workflow, where basic level cues play important roles
(\todo{examples}). Among there basic cues, camera geometry are
particularly useful for some computer vision tasks like facade
detection, indoor layout estimation~\cite{ren2016coarse}.

In the camera pin-hole model, camera geometry denotes the camera
both intrinsic and extrinsic parameters. The intrinsic parameters
encode the information of the camera itself such as focal length,
principal point \etc; the extrinsic parameters are referred to as the
external states of the camera, such as the camera poses and
offset/location under some coordinate system. In this work, our study
of the camera geometry mainly includes the extrinsic like camera pose
and the geo-orientation/geo-localization of the camera.

During past dacades, many research has been done to detect the camera
geometry from input images. \todo{citation and examples}. Most of the
previous work optimize for a limited set of camera calibration
parameters only. However, our research found that there exists an
effective way of optimizing multiple camera calibration parameters at
simultaneously (refer to \chapref{mcmc}). Our method use the Monte
Carlo Markov Chain (MCMC) method to sample the possible solutions that
minimize the objective function, which measure the ``matchness''
between the observation (input image) and the camera calibration.

Among the most influential approaches for camera geometry estimation,
most methods are based on traditional statistics diagram. Not until a
fewer years ago, machine learning based methods gradually caught
people's attention in this area, thanks for the popularity of deep
neural network. Neural networks were first proposed in 80th
(\todo{citation}), The back-forward method develop by Hinton
\todo{citation} makes the effective training possible. However neural
networks are generally thought by researchers merely toy that can only
solve a very limited set of tasks. It is the develop of computational
resources and the big collection of annotated data that make the
neural networks deeper and eventually bring it to the stage of
industrial applications.

In computer vision, low-level elements like edges, line segments,
super-pixels and vanishing points play important roles in many
applications like image measuration, facade detection, and
geo-localization.
Though the CNNs are good at extracting high-level information from
images, more and more researches start focusing on using CNN to
extract low-level these elements.
%
Lee \etal~\cite{lee2017semantic} propose to detect semantic line
segments, which are able to segment the scene semantically, using
neural network. Maninis \etal~\cite{maninis2016convolutional} and Yang
\etal~\cite{yang2016object} explore to detect object contours with
CNNs.
%
Aside from edges and contours, another category of low-level
information that the CNNs can detect is the camera geometry, like
camera location, vanishing points, and horizon lines.
%
In the area of vanishing point detection, we are among the first
researches that use deep neural networks to solve the problem. The
deep neural networks prove to be a good tool to extract high-level
information that offer great help to conquer the problems in camera
geometry estimation. \todo{many citations}.
%

\brief{
Geometric information can help training neural network to extract
high-level information.
}
Camera geometry can provide help learn high-level image semantics.
Hoiem \etal~\cite{hoiem2008putting} demonstrate that taking camera
perspective (horizon line) into consideration improves the performance
for object detection.

\todo{re-edit}
The use of overhead imagery has been proved useful for image
understanding.
Several methods have been recently proposed to jointly reason about
co-located aerial and ground image pairs. Luo
\etal~\cite{luo2008event} demonstrate that aerial imagery can aid
in recognizing the visual content of a geo-tagged ground image.
M{\'a}ttyus \etal~\cite{mattyus2016hd} perform joint inference over
monocular aerial imagery and stereo ground images for fine-grained
road segmentation. Wegner \etal~\cite{wegner2016cataloging} build a
map of street trees. Given the horizon line and the camera intrinsics,
Ghouaiel and Lef{\`e}vre~\cite{ghouaiel2016coupling} transform
geo-tagged ground-level panoramas to a top-down view to enable
comparisons with aerial imagery for the task of change detection.
Recent work on cross-view image
geolocalization~\cite{lin2013cross,lin2015learning,workman2015geocnn,workman2015wide}
 has shown that convolutional neural
networks are capable of extracting features from aerial imagery
that can be matched to features extracted from ground imagery.
Vo \etal~\cite{vo2016localizing} extend this line of work,
demonstrating improved geolocalization performance by applying an
auxiliary loss function to regress the ground-level camera orientation
with respect to the aerial image. To our knowledge, our work is the
first work to explore predicting the semantic layout of a ground
image from an aerial image.

\brief{
We use geometric information and DL to solve computer vision problems.
}


\section{Our Approach}
My thesis focuses on the image understanding with camera geometry.
There are two directions we will address: 1) How can we obtain camera
geometry from images; 2) How can we use camera geometry to improve the
current methods for image semantic extracting. For the first direction,
we explore both the traditional method and deep learning approaches on
different geometric calibration parameters. For the second direction,
we will exploit the image geometric location to help extract
semantically meaningful informations from images.

\begin{itemize}[noitemsep]

  \item \textbf{Camera Geometry Detection:} 
  We investigate different methods to detect camera geometry from the
  input images. We first explore a sampling approach to find the
  complete camera calibration parameters that fit the observation of
  objects (input image) best. In our next work, we use a deep neural
  network to learn to detect horizon line and vanishing points. These
  two approaches provides information for the next stage of our
  research.

  \item \textbf{Camera Geometry for Image Understanding:}
  \todo{more writing}
  We explore the potentials of camera geometry for image understanding.
  Current machine learning methods address problems that involves mostly
  the ground imagery. Tons of dataset targeting on ground imagery are
  annotated while much fewer are done for the overhead imagery.

\end{itemize}


\section{Synopsis}

The remainder of this work is organized as follows:
  
\begin{itemize}[noitemsep]

  \item \textbf{\chapref{mcmc} - 
  Complete Camera Geo-Calibration with MCMC Method:} \newline \newline
  The complete camera geo-calibration includes the detection of camera
  location in the world, the pose and the focal length of the camera
  given the input image. By constructing a objective function that
  measures the ``matchness'' between the camera calibration and the
  observation of the image, we are able to optimize the parameters in
  the calibration space. However, this space multi-dimensional, which
  makes the naive grid search infeasible. Our insight is to do the
  optimization using Markov Chain Monte Carlo (MCMC) method. Our
  algorithm is able to avoid unnecessary sampling in the
  low-probabilistic region in the parameter space thus reduce the
  computational time dramatically. \newline

  \item \textbf{\chapref{fasthorizon} -
  Detecting Horizon and VPs using CNN:} \newline \newline
  Horizon line and vanishing points (VPs) plays important roles in many
  application such as image mensuration and facade detection. In the
  previous work, researchers tended to use build bottom-up approaches to
  solve this problem. Our work is inspired by human's perception of the
  horizon line and the vanishing point. Our method exploit the image
  global context to provide a prior distribution of the final solution,
  thus to reduce the dimensionality of the solution space. \newline

  \item \textbf{\chapref{crosstransf} -
  Cross-view Domain Adaptation:} \newline \newline
  Current machine learning methods address problems that involves mostly
  the ground imagery. Tons of dataset targeting on ground imagery are
  annotated while much fewer are done for the overhead imagery.
  In this work, we try to transfer the semantics from the ground imagery
  to the overhead imagery by identifying the latent geometry
  correspondences between these two. Similar to methods that driven by
  the projection losses \todo{citation}, our network learns 
  both the geometric projection between the overhead and ground
  images, and the semantics of the overhead images jointly. \newline

  \item \textbf{\chapref{whenwhere} -
  Extract High-Level Feature by Learning When and Where:} \newline \newline
  One of the key to the success of machine learning approaches are the
  large quantity of annotated data. However, fully annotated imagery
  only consists of a tiny part of the image resources available online.
  The idea is to exploit other meta-info that are automatically
  recorded/captured by devices to train the ML models.  We demonstrate
  that the camera geometry can be used as weak ground truth for
  supervision problem. By learning the capture time and the geolocation
  of the images, our network is able capture the geo-temporal related
  high-level features. \newline

\end{itemize}
