\chapter{Introduction}
\label{chap:intro}

\makeatletter
\newenvironment{chapquote}[2][2em]
{\setlength{\@tempdima}{#1} \def\chapquote@author{#2} \parshape 1
  \@tempdima \dimexpr\textwidth-2\@tempdima\relax \itshape}
{\par\normalfont\hfill--\
\chapquote@author\hspace*{\@tempdima}\par\bigskip}
\makeatother

\begin{chapquote}{Alan Turing}
  ``I propose to consider the question: Can machine think?''
\end{chapquote}

Before the father of computer asked this question, creating
machines that can think as human has been a dream through the human
history. In the late 18th century, Hungarian inventor, Wolfgan von
Kempelen, constructed an automatic machine called ``The Turk'' (also
known as the ``Mechanical Turk'') to play a chess game. Though it
proved to be an elaborate hoax, people's enthusiasm for machine
intelligence lasts till today. As one of the core elements of
intelligence, human vision plays an important role in our daily lives.
In order to make computer understand images, scholars had put a lot of
research efforts in the study of computer vision. In 1960s, MIT
professor Dr. Marvin Minsky asked his undergraduates to develop
programs that can recognize objects from a scene in their summer
project~\cite{boden2006mind}. Half a century has passed, the question
of this ``summer project'' still remains open.

The ultimate goal of image understanding is to transfer the visual
images into numerical or symbolic descriptions of the world
that are helpful for other thought processes for decision making.
In general, understanding images is not trivial for machines.
Under different scenarios, researchers divide image
understanding into different computer vision tasks. For example,
pedestrian detection is essential to autonomous driving, and image
retrieval to an image searching engine.

As one of the primary vision tasks, camera calibration plays an
important role in many applications like autonomous driving, indoor
layout estimation and image mensuration. 
In the camera pin-hole model, the projection from scene to the image
frame are determined by camera intrinsic and extrinsic parameters. 
The intrinsic parameters describe the information of camera
itself such as focal length, principal point \etc; 
The extrinsic parameters describe the external states of the
camera, such as the camera poses and translation under some
coordinate system.
Some applications like GIS and geo-localization are more interested in
calibrating the camera under geographic coordinates, namely knowing
the camera pose with respect to the ground reference frame (\ie ENU)
and camera location in geographic coordinates. We refer to such
calibration as \textbf{geo-calibration}.
The complete geo-calibration includes intrinsic (focal
length, principal points \etc) and extrinsic parameters.
In this work, our study mainly focuses on estimating geo-calibration
extrinsic parameters like camera pose and the
geo-orientation/geo-localization of the camera.

In current work of camera calibration, most algorithms are
deterministic, which means they output fixed solutions. Deterministic
systems may seem straight-forward but they also forebear the following
drawbacks: 1) Deterministic systems can not handle uncertainties which
are commonly encountered in the real world problems.  2) Deterministic
systems are not friendly to applications developed with graphic models
that are interfaced with their outputs.
To address these drawbacks, We propose to build probabilistic
models for camera geo-calibration.
We first build a probabilistic model to detect multiple
geo-calibration parameters jointly.  Our algorithm treats each set of
geo-calibration parameters as a point in the solution space. It
constructs an energy function for points in the solution space to
measure the fitness between the geo-calibration parameters and the
observations (images), so that each point is associated with an energy
score.  The probability over the calibration parameters is
anti-proportional to the energy score, which we then compute with
graphic model approach. The essential part of our algorithm is the
construction of the energy function. To better measure the fitness
between the calibration parameters and the observation, we divide the
energy function into a combination of partial calibration problems,
each of which is responsible for a subset of calibration parameters.
We will discuss these constraint functions in several following work.
 
In recent years, deep neural networks (DNNs) rise as stars. Methods
using DNNs achieve state-of-the-art performances in
many computer vision areas (\todo{citations}). We demonstrate that
DNNs are good tools to solve partial calibration problems. 
As a next step of our study, We focus on calibrating cameras with deep
neural networks.

We first estimate camera roll and pitch angles. Assuming the intrinsic
parameters are known, camera roll and pitch angles can be
derived from the horizon line position in the image. Detecting horizon
line is easier because it is tightly related to the content/appearance
of the scene. In our work, we first create a new method using DNNs to
give a prior distribution of the horizon line, then assign a
probability to each by measuring their consistences with vanishing
points detected using traditional statistic methods.

Study of geo-localization has a long history. \todo{citations}. In
another work, We seek to geo-localize images with temporal
information. The system estimates the discrete probability over the
camera geographic location. This application works in rough scale. In
our next work, we design an algorithm that can do fine-scale localization
based on the rough geographic location inputs. In that work, we
designed a deep neural network to transfer the semantic segmentation
masks from the top-view perspective to the ground view perspective. In
order to train the network, we have to construct a data form called
cross-view image pairs.

Overhead imagery plays an import role in the cross-view learning area.
To construct a cross view image pair, we need to know the geographic
location (sometimes also with the geographic orientation).
\todo{rewrite the following} Efforts have been made to
automate overhead image analysis. As early as
1970~\cite{idelsohn1970learning} methods were introduced for
classifying terrain types from a single overhead image, with the goal
of automatically generating terrain maps.  Similarly, in 1976 Bajcsy
et al.~\cite{bajcsy1976computer} described a system for recognizing
roads, intersections, and other road-like objects in overhead imagery.
However, as overhead imaging differs drastically from ground-level
imaging, the majority of techniques that have been developed have
occurred independently and in task-specific ways~\cite{Rozen}.

Benefited from the fast growing monitoring satellite market, overhead
imagery grows fast nowadays. Microsoft BingMap provides public access
to their overhead image database with various resolution options.
Given the geographic information of the ground image, we are able to
download the corresponding overhead images to form the geo-orientation
aligned cross-view image pairs.

%Our algorithm takes advantage of the cross-view pairs, to train a deep
%neural network that can semantically segment the overhead image and
%then transfer the segmentation mask to the ground view. To finer
%geo-orient and localize the image, we can match the ground
%segmentation mask with the mask generated from overhead image at
%location $l$.

Another contribution of our work is that along the training for
networks that do geo-calibration, we are able to learn to extract
high-level features from the networks. \todo{expand}


\section{Our Approach}
We first propose primary probabilistic model to jointly estimate 
the full geo-calibration parameters. The key for the success of this
model is to construct good constraint functions that describe the
fitness between the calibration parameters and the images. We then
dismantle the full geo-calibration into several partial
geo-calibration tasks, each of which provide a constraint function to
fit in the primary model.

\begin{itemize}[noitemsep]
  \item \textbf{General Probabilistic Architecture for Geo-Calibration:}
  We design a graphic model to estimate the probability over
  full geo-calibration parameters. Our model is a general
  architecture, which can take different constraints to form
  the energy function.

  \item \textbf{Constraint Function for Camera Poses:}
  We develop a constraint function for two camera pose angles: pitch
  and roll.

  \item \textbf{Constraint Function for Camera Geographic Locations
  and Orientations:}
  We develop a constraint function for camera geographic locations and
  orientations.

\end{itemize}


\section{Synopsis}

The remainder of this work is organized as follows:
  
\begin{itemize}[noitemsep]

  \item \textbf{\chapref{mcmc} - 
  Complete Camera Geo-Calibration with MCMC Method:} \newline \newline
  The complete camera geo-calibration includes the detection of camera
  location in the world, the pose and the focal length of the camera
  given the input image. By constructing a objective function that
  measures the ``fitness'' between the camera calibration and the
  observation of the image, we are able to optimize the parameters in
  the calibration space. However, this space multi-dimensional, which
  makes the naive grid search infeasible. Our insight is to do the
  optimization using Markov Chain Monte Carlo (MCMC) method. Our
  algorithm is able to avoid unnecessary sampling in the
  low-probabilistic region in the parameter space thus reduce the
  computational time dramatically. \newline

  \item \textbf{\chapref{fasthorizon} -
  Detecting Pitch and Roll using CNN:} \newline \newline
  Assuming the intrinsic parameters are known, camera roll and pitch
  angles can be derived from the horizon line position in the image.
  Horizon line and vanishing points (VPs) plays important roles in many
  application such as image mensuration and facade detection. In the
  previous work, researchers tended to use build bottom-up approaches to
  solve this problem. Our work is inspired by human's perception of the
  horizon line and the vanishing point. Our method exploit the image
  global context to provide a prior distribution of the final solution,
  thus to reduce the dimensionality of the solution space. \newline

  \item \textbf{\chapref{whenwhere} -
  Estimating Geographic Location with Temporal Information:} \newline \newline
  In this work, we demonstrate that we can estimate the camera geographic
  location with the input image and its temporal information.
  One of the key to the success of machine learning approaches are the
  large quantity of annotated data. However, fully annotated imagery
  only consists of a tiny part of the image resources available online.
  The idea is to exploit other meta-info that are automatically
  recorded/captured by devices to train the ML models. We demonstrate
  that the camera geometry can be used as weak ground truth for
  supervision problem. By learning the capture time and the geolocation
  of the images, our network is able capture the geo-temporal related
  high-level features. \newline

  \item \textbf{\chapref{crosstransf} -
  Finer Localization by Matching the Overhead Images:} \newline \newline
  With the rough geographic location, we are able to refine the result
  with overhead images.
  Current machine learning methods address problems that involves mostly
  the ground imagery. Tons of dataset targeting on ground imagery are
  annotated while much fewer are done for the overhead imagery.
  In this work, we try to transfer the semantics from the ground imagery
  to the overhead imagery by identifying the latent geometry
  correspondences between these two. Similar to methods that driven by
  the projective losses \todo{citation}, our network learns 
  both the geometric projection between the overhead and ground
  images, and the semantics of the overhead images jointly. \newline


\end{itemize}
