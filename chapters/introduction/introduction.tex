\chapter{Introduction}
\label{chap:intro}

\makeatletter
\newenvironment{chapquote}[2][2em]
{\setlength{\@tempdima}{#1} \def\chapquote@author{#2} \parshape 1
  \@tempdima \dimexpr\textwidth-2\@tempdima\relax \itshape}
{\par\normalfont\hfill--\
\chapquote@author\hspace*{\@tempdima}\par\bigskip}
\makeatother

\begin{chapquote}{Alan Turing}
  ``I propose to consider the question: Can machine think?''
\end{chapquote}

Before the father of computer asked this question, creating
machines that can think as human has been a dream through the human
history. In the late 18th century, Hungarian inventor, Wolfgan von
Kempelen, constructed an automatic machine called ``The Turk'' (also
known as the ``Mechanical Turk'') to play a chess game. Though it
proved to be an elaborate hoax, people's enthusiasm for machine
intelligence lasts till today. As one of the core elements of
intelligence, human vision plays an important role in our daily lives.
In order to make computer understand images, scholars had put a lot of
research efforts in the study of computer vision. In 1960s, MIT
professor Dr. Marvin Minsky asked his undergraduates to develop
programs that can recognize objects from a scene in their summer
project~\cite{boden2006mind}. Half a century has passed, the question
of this ``summer project'' still remains open.

The ultimate goal of image understanding is to transfer the visual
images into numerical or symbolic descriptions of the world
that are helpful for other thought processes for decision making.
In general, understanding images is not trivial for machines.
Under different scenarios, researchers divide image
understanding into different computer vision tasks. For example,
pedestrian detection is essential to autonomous driving, and image
retrieval to an image searching engine.

As one of the primary vision tasks, camera calibration plays an
important role in many applications like indoor layout estimation and
image mensuration. 
In the camera pin-hole model, camera calibration parameters consist of
the camera intrinsics and the extrinsic. The intrinsic parameters of
a camera encode information of the camera itself such as focal length,
principal point \etc; the extrinsic parameters are referred to as the
external states of the camera, such as the camera poses and
offset/location under some coordinate system. Specifically,
camera geo-calibration consider the camera locations in the geographic
coordinates.
In this work, our study mainly focus on the extrinsic like camera pose
and the geo-orientation/geo-localization of the camera.
 
In current work of camera calibration, most algorithms are
deterministic, which means they output fixed solutions. Applications
that takes probabilistic inputs (especially for those which use
graphic model) don't like it. To address this, we propose to build
probabilistic models for camera geo-calibration using deep neural
networks.

A complete geo-calibration detect both camera intrinsic (focal length,
principal points \etc) and extrinsic parameters, which includes camera
roll, pitch angles, geographic orientation and geographic locations.
In our work, we first build a probabilistic model to detect multiple
geo-calibration parameters jointly. Our algorithm construct a set of
constraint functions to measure the fitness between the
geo-calibration parameters and the observations. An energy function
is constructed with these constraint functions. Thus each point in the
solution space forebear a energy score (proportional to the
probability at that point). We then use graphic model approach to
sample these point in the space.

In recent years, deep neural networks (DNNs) rise as stars in recent
years. Methods that uses DNNs achieve state-of-the-art performances in
many computer vision areas (\todo{citations}). We demonstrate that
DNNs are good tools to solve the calibration problems. 
As a next step of our study, We further
divide these constraints on subset of the calibration parameters and
conquer them with DNNs.

We first focus on camera roll and pitch angles. Given the already
known intrinsic parameters, camera roll and pitch angles can be
derived from the horizon line position in the image. Detecting horizon
line is easier because it is tightly related to the content/appearance
of the scene. In our work, we create a new method to use DNNs to give
a prior distribution of the horizon line, then assign a probability to
each by measuring their consistences with vanishing points detected
with traditional statistic methods.

Study of geo-localization has a long history. 


\section{Our Approach}
My thesis focuses on the image understanding with camera geometry.
There are two directions we will address: 1) How to estimate camera
geometry from images; 2) How can we use camera geometry to improve the
current methods for image semantic extracting. For the first direction,
we explore both the traditional method and deep learning approaches on
different geometric calibration parameters. For the second direction,
we will exploit the image geometric location to help extract
semantically meaningful informations from images.

\begin{itemize}[noitemsep]

  \item \textbf{Camera Geometry Detection:} 
  We investigate different methods to detect camera geometry from the
  input images. We first explore a sampling approach to find the
  complete camera calibration parameters that fit the observation of
  objects (input image) best. In our next work, we use a deep neural
  network to learn to detect horizon line and vanishing points. These
  two approaches provides information for the next stage of our
  research.

  \item \textbf{Camera Geometry for Image Understanding:}
  We explore the potentials of camera geometry for image understanding.
  Current machine learning methods address problems that involves mostly
  the ground imagery. 
  Studies around overhead images are relatively less thus much fewer
  annotated data exists in public. With the camera geometric
  information, we can pair the ground imagery with overhead imagery to
  form cross-view pairs. Our algorithm explore how to transfer the
  learned knowledge from a network designed for ground level image
  semantic segmentation to a new network designed for overhead imagery
  semantic segmentation use these cross-view pairs.
  Furthermore, we also explore the potential of the camera geometry as
  weak annotation in supervised learning. Our algorithm shows that by
  directly learning to predict camera locations, the network networks
  are able to extract high-level geo-temporal features.

\end{itemize}


\section{Synopsis}

The remainder of this work is organized as follows:
  
\begin{itemize}[noitemsep]

  \item \textbf{\chapref{mcmc} - 
  Complete Camera Geo-Calibration with MCMC Method:} \newline \newline
  The complete camera geo-calibration includes the detection of camera
  location in the world, the pose and the focal length of the camera
  given the input image. By constructing a objective function that
  measures the ``matchness'' between the camera calibration and the
  observation of the image, we are able to optimize the parameters in
  the calibration space. However, this space multi-dimensional, which
  makes the naive grid search infeasible. Our insight is to do the
  optimization using Markov Chain Monte Carlo (MCMC) method. Our
  algorithm is able to avoid unnecessary sampling in the
  low-probabilistic region in the parameter space thus reduce the
  computational time dramatically. \newline

  \item \textbf{\chapref{fasthorizon} -
  Detecting Horizon and VPs using CNN:} \newline \newline
  Horizon line and vanishing points (VPs) plays important roles in many
  application such as image mensuration and facade detection. In the
  previous work, researchers tended to use build bottom-up approaches to
  solve this problem. Our work is inspired by human's perception of the
  horizon line and the vanishing point. Our method exploit the image
  global context to provide a prior distribution of the final solution,
  thus to reduce the dimensionality of the solution space. \newline

  \item \textbf{\chapref{crosstransf} -
  Cross-view Domain Adaptation:} \newline \newline
  Current machine learning methods address problems that involves mostly
  the ground imagery. Tons of dataset targeting on ground imagery are
  annotated while much fewer are done for the overhead imagery.
  In this work, we try to transfer the semantics from the ground imagery
  to the overhead imagery by identifying the latent geometry
  correspondences between these two. Similar to methods that driven by
  the projective losses \todo{citation}, our network learns 
  both the geometric projection between the overhead and ground
  images, and the semantics of the overhead images jointly. \newline

  \item \textbf{\chapref{whenwhere} -
  Extract High-Level Feature by Learning When and Where:} \newline \newline
  One of the key to the success of machine learning approaches are the
  large quantity of annotated data. However, fully annotated imagery
  only consists of a tiny part of the image resources available online.
  The idea is to exploit other meta-info that are automatically
  recorded/captured by devices to train the ML models.  We demonstrate
  that the camera geometry can be used as weak ground truth for
  supervision problem. By learning the capture time and the geolocation
  of the images, our network is able capture the geo-temporal related
  high-level features. \newline

\end{itemize}
