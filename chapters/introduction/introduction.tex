\chapter{Introduction}
\label{chap:intro}

\makeatletter
\newenvironment{chapquote}[2][2em]
{\setlength{\@tempdima}{#1} \def\chapquote@author{#2} \parshape 1
  \@tempdima \dimexpr\textwidth-2\@tempdima\relax \itshape}
{\par\normalfont\hfill--\
\chapquote@author\hspace*{\@tempdima}\par\bigskip}
\makeatother

\begin{chapquote}{Alan Turing}
  ``I propose to consider the question: Can machine think?''
\end{chapquote}

Before the father of computer asked this question, creating
machines that can think as human has been a dream through the human
history. In 1960s, MIT professor Dr. Marvin Minsky asked his
undergraduates to develop programs that can recognize objects from a
scene in their summer project~\cite{boden2006mind}. Half a century has
passed, the question of this ``summer project'' still remains open.

The ultimate goal of image understanding is to transfer the visual
signals (images/videos) into abstract symbolic descriptions of the
world which are helpful for decision making.
In general, understanding images is not trivial for machines.
Under different scenarios, researchers divide image
understanding into different computer vision tasks. For example,
pedestrian detection is essential to autonomous driving, and image
retrieval to an image searching engine.

For many applications, knowing when, where, and in which direction a
picture was taken is important to understand the world. We refer to
this such a vision task as {\em geo-calibration}.
In current work of geo-calibration, most algorithms are
deterministic, which means they output fixed solutions. Deterministic
systems may seem straight-forward but they also have some 
drawbacks: 1) Deterministic systems can not handle uncertainties in
the real world very well. 2) they are not
friendly to applications developed with graphic models that are
interfaced with their outputs. To address these problems, We propose
to build probabilistic models for camera geo-calibration.

Beside that, we also explored the possibility to learn high-level
features from the networks that do geo-calibration. \todo{expand}

\section{Background}

\subsection{Geo-Calibration}
Automatic image geo-calibration continues to grow in importance as a
direct result of the increasing amount of imagery available
via the Internet. Conceptually geo-calibration task is
straightforward: given an image, identify the location it was captured
in the world and the direction the camera was facing. Solving this
problem is of great value for a wide variety of fields, with potential
applications ranging from the forensic sciences~\cite{stylianou13jane}
to crowd-sourced environmental monitoring~\cite{zhang2012mining}.

Extracting location-dependent or orientation-dependent features from
image data has drawn a great detail of attention from the vision
community~\cite{jacobs07geolocate, jacobs11geolocate,
jacobs08geoorient}. The common trend amongst these methods is that
they take advantage of a large dataset of geo-referenced images. Hays
and Efros~\cite{hays2008im2gps} use a data-driven scene matching
approach to localize a query image using a large dataset of geo-tagged
images.  Doersch et al. extract location-dependent features that
capture the relative appearance differences of large cities.  Lin et
al.~\cite{lin2013cross} localize a ground-level image by learning the
relationship between pairs of ground and aerial images of the same
location. Other techniques focus on urban environments and infer
location using local image
descriptors~\cite{schindler2008detecting,snavely2006photo}. Li et
al.~\cite{li2012worldwide} exploit geo-registered 3D points clouds to
estimate camera pose. Many other cues exist, such as the
skyline~\cite{baatz2012large,ramalingam2009geolocalization}, sky
appearance~\cite{lalonde2010sun,workman2014rainbow}, and
shadows~\cite{junejo2008estimating,wu2010geo}.

However, recognizing the geo-location and geo-orientation of an
arbitrary outdoor image is an extremely challenging task.  Many
methods have been proposed; the most common approach is to build a
large database of images with known location and localize a query
image using either local~\cite{li2010location,schindler2008detecting}
or global~\cite{hays2008im2gps,doersch2012what} image features.  This
approach is not applicable when no nearby ground-level imagery exists
in the reference database, such as when the image was not captured
near a popular tourist destination.  Even when reference imagery is
available, the appearance of the objects may not be visually
distinctive, for example a train track or a body of water. 

\subsection{Deep Neural Networks for Geo-Calibration}
In recent years, deep neural networks (DNNs) rise as stars. Methods
using DNNs achieve state-of-the-art performances in
many computer vision areas (\todo{citations}). 
Recent work on cross-view image
geolocalization~\cite{lin2013cross,lin2015learning,workman2015geocnn,workman2015wide}
 has shown that convolutional neural
networks are capable of extracting features from aerial imagery
that can be matched to features extracted from ground imagery.
Vo \etal~\cite{vo2016localizing} extend this line of work,
demonstrating improved geolocalization performance by applying an
auxiliary loss function to regress the ground-level camera orientation
with respect to the aerial image.


\subsection{Overhead Imagery}
Overhead imagery plays an import role in the cross-view learning area.
To construct a cross view image pair, we need to know the geographic
location (sometimes also with the geographic orientation).
\todo{rewrite the following} Efforts have been made to
automate overhead image analysis. As early as
1970~\cite{idelsohn1970learning} methods were introduced for
classifying terrain types from a single overhead image, with the goal
of automatically generating terrain maps.  Similarly, in 1976 Bajcsy
et al.~\cite{bajcsy1976computer} described a system for recognizing
roads, intersections, and other road-like objects in overhead imagery.
However, as overhead imaging differs drastically from ground-level
imaging, the majority of techniques that have been developed have
occurred independently and in task-specific ways~\cite{Rozen}.

Benefited from the fast growing monitoring satellite market, overhead
imagery grows fast nowadays. Microsoft BingMap provides public access
to their overhead image database with various resolution options.
Given the geographic information of the ground image, we are able to
download the corresponding overhead images to form the geo-orientation
aligned cross-view image pairs.

\section{Our Approach}
We divide our approach in two steps. First, we propose a general
probabilistic model to jointly estimate the geo-calibration
parameters. The key for the success of this model is to construct good
constraint functions that describe the fitness between the calibration
parameters and the images. Then we dismantle the full geo-calibration
into several partial geo-calibration tasks, each of which provide a
constraint function that could fit into our general model.

\subsection{General Probabilistic Model for Geo-Calibration}
The complete camera geo-calibration includes the detection of camera
geographic location, the pose and the focal length of the camera.  We
design a graphic model to estimate the probability over these
geo-calibration parameters. 
We construct an scoring function to measure the ``fitness'' between
the calibration parameters and the observation of the image. The
scoring function is proportional to the probability distribution of
the camera parameters. We propose a Markov Chain Monte Carlo (MCMC)
sampling approach to approximate the underlying probability
distribution over the full geo-calibration of the camera.
Our model is a general architecture, which can take different
constraints to form the energy function. The details about this model
can be found in \chapref{mcmc}.

\subsection{Developing Constraint Functions}
In our general model, the scoring function is defined as a linear
combination of multiple constraint functions. We then develop
these constraint functions to describe different calibration
parameters.

\begin{itemize}[noitemsep]
  \item \textbf{Constraint for Horizon Line and Vanishing Points:}
  We first estimate camera roll and pitch angles. Assuming the intrinsic
  parameters are known, camera roll and pitch angles can be
  derived from the horizon line position in the image. Detecting horizon
  line is easier because it is tightly related to the content/appearance
  of the scene. In our work, we first create a new method using DNNs to
  give a prior distribution of the horizon line, then assign a
  probability to each by measuring their consistences with vanishing
  points detected using traditional statistic methods. \newline

  \item \textbf{Constraint for Coarse Geographic Locations:}
  In this work, we demonstrate that we can estimate the camera geographic
  location with the input image and its temporal information.
  One of the key to the success of machine learning approaches are the
  large quantity of annotated data. However, fully annotated imagery
  only consists of a tiny part of the image resources available online.
  The idea is to exploit other meta-info that are automatically
  recorded/captured by devices to train the ML models. We demonstrate
  that the camera geometry can be used as weak ground truth for
  supervision problem. By learning the capture time and the geolocation
  of the images, our network is able capture the geo-temporal related
  high-level features. \newline

  \item \textbf{Constraint for Fine Geographic Locations and Orientations}
  With the rough geographic location, we are able to refine the result
  with overhead images.
  Current machine learning methods address problems that involves mostly
  the ground imagery. Tons of dataset targeting on ground imagery are
  annotated while much fewer are done for the overhead imagery.
  In this work, we try to transfer the semantics from the ground imagery
  to the overhead imagery by identifying the latent geometry
  correspondences between these two. Similar to methods that driven by
  the projective losses \todo{citation}, our network learns 
  both the geometric projection between the overhead and ground
  images, and the semantics of the overhead images jointly. \newline

\end{itemize}
