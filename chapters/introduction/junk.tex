Hold-Geoffroy \etal~\cite{hold2017perceptual}
improve~\cite{workman2016horizon}'s results by using deeper networks,
and introduce a new perceptual measure for horizon line detection.
Unlike the previous work that mainly focus on horizon line detection,
Zhang \etal~\cite{zhang2018dominant} propose to detect the dominant
vanishing point using deep neural networks.

\brief{
Geometric information can help training neural network to extract
high-level information.
}

\todo{re-edit}
The use of overhead imagery has been proved useful for image
understanding.
Several methods have been recently proposed to jointly reason about
co-located aerial and ground image pairs. Luo
\etal~\cite{luo2008event} demonstrate that aerial imagery can aid
in recognizing the visual content of a geo-tagged ground image.
M{\'a}ttyus \etal~\cite{mattyus2016hd} perform joint inference over
monocular aerial imagery and stereo ground images for fine-grained
road segmentation. Wegner \etal~\cite{wegner2016cataloging} build a
map of street trees. Given the horizon line and the camera intrinsics,
Ghouaiel and Lef{\`e}vre~\cite{ghouaiel2016coupling} transform
geo-tagged ground-level panoramas to a top-down view to enable
comparisons with aerial imagery for the task of change detection.
Recent work on cross-view image
geolocalization~\cite{lin2013cross,lin2015learning,workman2015geocnn,workman2015wide}
 has shown that convolutional neural
networks are capable of extracting features from aerial imagery
that can be matched to features extracted from ground imagery.
Vo \etal~\cite{vo2016localizing} extend this line of work,
demonstrating improved geolocalization performance by applying an
auxiliary loss function to regress the ground-level camera orientation
with respect to the aerial image. To our knowledge, our work is the
first work to explore predicting the semantic layout of a ground
image from an aerial image.


The study of understanding images has been around for half a century.
A well known anecdote in computer vision community happened in 1960s,
when the MIT professor Dr. Marvin Minsky assigned a summer programming
project to his undergraduates, to recognize objects in a
scene~\cite{boden2006mind}. 
%
Similar to the story of the famous ``Four Color Conjecture'', where
one of the world's renowned mathematicians Dr. Hermann Minkowski
claimed in his class that he was able to solve it before the class
ends.  
However, Not until one hundred years later since it was first
proposed, the ``Four Color Conjecture'' has been stamped as theorem by
the continuous efforts of several generations of talented
mathematicians.  While on the other side, Dr. Marvin Minsky's summer
program still remains open.


To achieve this
goal, researchers adopt a divide-and-conquer strategy, that is
dividing image understanding into various sub-problems like object
recognition, semantic segmentation, and pose estimation \etc.  During
past decades, a lot of methods are developed to solve these
sub-problems. Many of them adopted a bottom-up (\todo{citations})
workflow, where basic level cues play important roles
(\todo{examples}). Among these basic cues, camera geometry is
particularly useful for some computer vision tasks like facade
detection, indoor layout estimation~\cite{ren2016coarse}.

 
In the camera pin-hole model, camera geometric parameters consists of
the camera intrinsics and the extrinsics. The intrinsic parameters of
a camera encode information of the camera itself such as focal length,
principal point \etc; the extrinsic parameters are referred to as the
external states of the camera, such as the camera poses and
offset/location under some coordinate system. In this work, our study
of the camera geometry mainly includes the extrinsic like camera pose
and the geo-orientation/geo-localization of the camera.

\sepline

Among algorithms that are developed to address these sub-tasks, deep
neural networks (DNNs) rise as stars in recent years. Methods that
uses DNNs achieve state-of-the-art performances in many computer
vision areas (\todo{citations}). As parametric algorithms, deep neural
networks require training for their parameters. Methods to train DNNs
include supervised algorithms, unsupervised algorithms, and
reinforcement learning. Supervised algorithms are the most popular and
commonly applied in many computer vision areas, thanks to their good
performances and the fact that they are easy to implement. However,
supervised learning approaches are sensitive to the quality of data
annotations. In some applications like overhead image segmentation and
transient attributes estimation, data with noise-free annotation is
either limited in quantity or technically difficult to acquire.
In this thesis, Our methods focus on lavaging the camera geographic
information to deal with the challenges caused by the lack of
noise-free annotations during DNN training. The insight is
that the camera geographic information encodes the semantic
information because it is related to the appearances of
the scenes.

The process of estimating the camera geographic information is
referred to as geo-calibration. The goal is to estimate the geographic
location and the geo-orientation of the given camera. 
Compared to the common camera calibration, geo-calibration focuses on
calibrating the camera in the geographic coordinates. 

Geographic
information reveals the scene appearances 

\sepline

Many research has been done to detect the camera
geometry from input images. \todo{citation and examples}. Most of the
previous work optimize for a limited set of camera calibration
parameters only. However, our research found that there exists an
effective way of optimizing multiple camera calibration parameters at
simultaneously (refer to \chapref{mcmc}). Our algorithm uses the Monte
Carlo Markov Chain (MCMC) method to sample the possible solutions that
minimize the objective function, which measures the ``matchness''
between the observation (input image) and the camera calibration.

Among the most influential approaches for camera geometry estimation,
most methods are based on traditional statistics diagram. Not until a
fewer years ago, machine learning based methods gradually caught
people's attention in this area, thanks for the popularity of deep
neural network. Neural networks were first proposed in 80th
(\todo{citation}), The back-forward method developed by Hinton
\todo{citation} makes the effective training possible. However, neural
networks are generally thought by researchers merely toy that can only
solve a very limited set of tasks. It is the develop of computational
resources and the big collection of annotated data that enables the
neural networks to grow deeper and eventually brings it to the stage of
industrial applications.

In computer vision, low-level elements like edges, line segments,
super-pixels and vanishing points play important roles in many
applications like image measuration, facade detection, and
geo-localization.
Though the CNNs are good at extracting high-level information from
images, more and more researches start focusing on using CNN to
extract low-level these elements.
%
Lee \etal~\cite{lee2017semantic} propose to detect semantic line
segments, which are able to segment the scene semantically, using
neural network. Maninis \etal~\cite{maninis2016convolutional} and Yang
\etal~\cite{yang2016object} explore to detect object contours with
CNNs.
%
Aside from edges and contours, another category of low-level
information that the CNNs can detect is the camera geometry, like
camera location, vanishing points, and horizon lines.
%
In the area of vanishing point detection, we are among the first
researches that use deep neural networks to solve the problem. The
deep neural networks prove to be a good tool to extract high-level
information that offer great help to conquer the problems in camera
geometry estimation. \todo{many citations}.

In our previous work, we have shown that deep neural networks are
powerful tools for camera geometry estimation as well. In fact, the
camera geometry can also assist learning high-level image semantics.
Hoiem \etal~\cite{hoiem2008putting} demonstrate that taking camera
perspective (horizon line) into consideration improves the performance
for object detection. Godard \etal~\cite{godard2017unsupervised} learn
to predict the scene depth from a single image using left-right
consistency, which is also an application of camera geometry.
\todo{more citations about the projective loss}.

Another example of camera geometry boosting the learning project is to
exploit the overhead imagery. Using the geometric locations of the
ground-view images we can download the corresponding overhead images
at the same locations through public online resources like bingmap
\todo{cite}. We refer to these image pairs as ``cross-view image
pairs''.

\todo{copy from scott} Efforts have been made to
automate overhead image analysis. As early as
1970~\cite{idelsohn1970learning} methods were introduced for
classifying terrain types from a single overhead image, with the goal
of automatically generating terrain maps.  Similarly, in 1976 Bajcsy
et al.~\cite{bajcsy1976computer} described a system for recognizing
roads, intersections, and other road-like objects in overhead imagery.
However, as overhead imaging differs drastically from ground-level
imaging, the majority of techniques that have been developed have
occurred independently and in task-specific ways~\cite{Rozen}.
%\brief{overhead image grows fast}.

Our work demonstrates that we are able to transfer the learned semantic
knowledge from the ground image domain to the overhead image domain,
given the camera geometric information of these two kind of images. In
our algorithm, the camera geometry helps to collect the ``cross-view
image pairs'' and aligned the orientation directions of images in a
pair.

Camera geometry can also be used as weak annotations during training
of the networks for image understanding.  One of the key to the
success of machine learning approaches are the large quantity of
annotated data. However, fully annotated imagery consists of only a
tiny part of the image resources available online. The idea is to
exploit other meta-info that are automatically recorded/captured by
devices to train the ML models. One of the meta-info potentially
helpful for learning is camera geometry.  We demonstrate that the
camera geometry can be used as weak ground truth for supervision
problem. By learning the capture time and the geolocation of the
images simultaneously, our network is able capture the geo-temporal
related high-level features.

My thesis focuses on the image understanding with camera geometry.
There are two directions we will address: 1) How to estimate camera
geometry from images; 2) How can we use camera geometry to improve the
current methods for image semantic extracting. For the first direction,
we explore both the traditional method and deep learning approaches on
different geometric calibration parameters. For the second direction,
we will exploit the image geometric location to help extract
semantically meaningful informations from images.

\begin{itemize}[noitemsep]

  \item \textbf{Camera Geometry Detection:} 
  We investigate different methods to detect camera geometry from the
  input images. We first explore a sampling approach to find the
  complete camera calibration parameters that fit the observation of
  objects (input image) best. In our next work, we use a deep neural
  network to learn to detect horizon line and vanishing points. These
  two approaches provides information for the next stage of our
  research.

  \item \textbf{Camera Geometry for Image Understanding:}
  We explore the potentials of camera geometry for image understanding.
  Current machine learning methods address problems that involves mostly
  the ground imagery. 
  Studies around overhead images are relatively less thus much fewer
  annotated data exists in public. With the camera geometric
  information, we can pair the ground imagery with overhead imagery to
  form cross-view pairs. Our algorithm explore how to transfer the
  learned knowledge from a network designed for ground level image
  semantic segmentation to a new network designed for overhead imagery
  semantic segmentation use these cross-view pairs.
  Furthermore, we also explore the potential of the camera geometry as
  weak annotation in supervised learning. Our algorithm shows that by
  directly learning to predict camera locations, the network networks
  are able to extract high-level geo-temporal features.
\end{itemize}


In the late 18th century, Hungarian inventor, Wolfgan von
Kempelen, constructed an automatic machine called ``The Turk'' (also
known as the ``Mechanical Turk'') to play a chess game. Though it
proved to be an elaborate hoax, people's enthusiasm for machine
intelligence lasts till today. As one of the core elements of
intelligence, human vision plays an important role in our daily lives.
In order to make computer understand images, scholars had put a lot of
research efforts in the study of computer vision. 


As one of the primary vision tasks, camera calibration plays an
important role in many applications like autonomous driving, indoor
layout estimation and image mensuration. 


In current work of camera calibration, most algorithms are
deterministic, which means they output fixed solutions. Deterministic
systems may seem straight-forward but they also forebear the following
drawbacks: 1) Deterministic systems can not handle uncertainties which
are commonly encountered in the real world problems.  2) Deterministic
systems are not friendly to applications developed with graphic models
that are interfaced with their outputs.
To address these drawbacks, We propose to build probabilistic
models for camera geo-calibration.
We first build a probabilistic model to detect multiple
geo-calibration parameters jointly.  Our algorithm treats each set of
geo-calibration parameters as a point in the solution space. It
constructs an energy function for points in the solution space to
measure the fitness between the geo-calibration parameters and the
observations (images), so that each point is associated with an energy
score.  The probability over the calibration parameters is
anti-proportional to the energy score, which we then compute with
graphic model approach. The essential part of our algorithm is the
construction of the energy function. To better measure the fitness
between the calibration parameters and the observation, we divide the
energy function into a combination of partial calibration problems,
each of which is responsible for a subset of calibration parameters.
We will discuss these constraint functions in several following work.


However, this space multi-dimensional, which
makes the naive grid search infeasible. Our insight is to do the
optimization using Markov Chain Monte Carlo (MCMC) method. Our
algorithm is able to avoid unnecessary sampling in the
low-probabilistic region in the parameter space thus reduce the
computational time dramatically. 


  Study of geo-localization has a long history. \todo{citations}. In
  another work, We seek to geo-localize images with temporal
  information. The system estimates the discrete probability over the
  camera geographic location. This application works in rough scale. In
  our next work, we design an algorithm that can do fine-scale localization
  based on the rough geographic location inputs. In that work, we
  designed a deep neural network to transfer the semantic segmentation
  masks from the top-view perspective to the ground view perspective. In
  order to train the network, we have to construct a data form called
  cross-view image pairs. \newline



\section{Synopsis}

The remainder of this work is organized as follows:
  
\begin{itemize}[noitemsep]

  \item \textbf{\chapref{mcmc} - 
  Complete Camera Geo-Calibration with MCMC Method:} \newline \newline
  The complete camera geo-calibration includes the detection of camera
  location in the world, the pose and the focal length of the camera
  given the input image. By constructing a objective function that
  measures the ``fitness'' between the camera calibration and the
  observation of the image, we are able to optimize the parameters in
  the calibration space. However, this space multi-dimensional, which
  makes the naive grid search infeasible. Our insight is to do the
  optimization using Markov Chain Monte Carlo (MCMC) method. Our
  algorithm is able to avoid unnecessary sampling in the
  low-probabilistic region in the parameter space thus reduce the
  computational time dramatically. \newline

  \item \textbf{\chapref{fasthorizon} -
  Detecting Pitch and Roll using CNN:} \newline \newline
  Assuming the intrinsic parameters are known, camera roll and pitch
  angles can be derived from the horizon line position in the image.
  Horizon line and vanishing points (VPs) plays important roles in many
  application such as image mensuration and facade detection. In the
  previous work, researchers tended to use build bottom-up approaches to
  solve this problem. Our work is inspired by human's perception of the
  horizon line and the vanishing point. Our method exploit the image
  global context to provide a prior distribution of the final solution,
  thus to reduce the dimensionality of the solution space. \newline

  \item \textbf{\chapref{whenwhere} -
  Estimating Geographic Location with Temporal Information:} \newline \newline
  In this work, we demonstrate that we can estimate the camera geographic
  location with the input image and its temporal information.
  One of the key to the success of machine learning approaches are the
  large quantity of annotated data. However, fully annotated imagery
  only consists of a tiny part of the image resources available online.
  The idea is to exploit other meta-info that are automatically
  recorded/captured by devices to train the ML models. We demonstrate
  that the camera geometry can be used as weak ground truth for
  supervision problem. By learning the capture time and the geolocation
  of the images, our network is able capture the geo-temporal related
  high-level features. \newline

  \item \textbf{\chapref{crosstransf} -
  Finer Localization by Matching the Overhead Images:} \newline \newline
  With the rough geographic location, we are able to refine the result
  with overhead images.
  Current machine learning methods address problems that involves mostly
  the ground imagery. Tons of dataset targeting on ground imagery are
  annotated while much fewer are done for the overhead imagery.
  In this work, we try to transfer the semantics from the ground imagery
  to the overhead imagery by identifying the latent geometry
  correspondences between these two. Similar to methods that driven by
  the projective losses \todo{citation}, our network learns 
  both the geometric projection between the overhead and ground
  images, and the semantics of the overhead images jointly. \newline


\end{itemize}


%Our algorithm takes advantage of the cross-view pairs, to train a deep
%neural network that can semantically segment the overhead image and
%then transfer the segmentation mask to the ground view. To finer
%geo-orient and localize the image, we can match the ground
%segmentation mask with the mask generated from overhead image at
%location $l$.


In the camera pin-hole model, the projection from scene to the image
frame are determined by the camera intrinsic and extrinsic parameters. 
The intrinsic parameters describe the information of camera
itself such as focal length, principal point \etc; 
The extrinsic parameters describe the external states of the
camera, such as the camera poses and translation under some
coordinate system.
Some applications like GIS and geo-localization are more interested in
calibrating the camera under geographic coordinates, namely knowing
the camera pose with respect to the ground reference frame (\ie ENU)
and camera location in geographic coordinates. We refer to such
calibration task as \textbf{geo-calibration}.

The complete geo-calibration includes intrinsic (focal
length, principal points \etc) and extrinsic parameters.
In this work, our study mainly focuses on estimating geo-calibration
extrinsic parameters like camera pose and the
geo-orientation/geo-localization of the camera.



%\subsection{Overhead Imagery}
%Overhead imagery plays an import role in the cross-view learning area.
%To construct a cross view image pair, we need to know the geographic
%location (sometimes also with the geographic orientation).
%\todo{rewrite the following} Efforts have been made to
%automate overhead image analysis. As early as
%1970~\cite{idelsohn1970learning} methods were introduced for
%classifying terrain types from a single overhead image, with the goal
%of automatically generating terrain maps.  Similarly, in 1976 Bajcsy
%et al.~\cite{bajcsy1976computer} described a system for recognizing
%roads, intersections, and other road-like objects in overhead imagery.
%However, as overhead imaging differs drastically from ground-level
%imaging, the majority of techniques that have been developed have
%occurred independently and in task-specific ways~\cite{Rozen}.

%Benefited from the fast growing monitoring satellite market, overhead
%imagery grows fast nowadays. Microsoft BingMap provides public access
%to their overhead image database with various resolution options.
%Given the geographic information of the ground image, we are able to
%download the corresponding overhead images to form the geo-orientation
%aligned cross-view image pairs.



Before the father of computer asked this question, creating
machines that can think as human has been a dream through the human
history. In 1960s, MIT professor Dr. Marvin Minsky asked his
undergraduates to develop programs that can recognize objects from a
scene in their summer project~\cite{boden2006mind}. Half a century has
passed, the question of this ``summer project'' still remains open.

%Beside geo-calibration, this thesis also studies the possibilities of
%learning high-level image representations with the help of geographic
%information. In one of our work, we demonstrate that we are able to
%learn geo-temporal feature representations of image with geographic
%locations and image timestamps. In another work, we exploit image geographic
%locations to transfer the learned knowledge of a CNN for ground image
%semantic segmentation to another CNN designed for overhead image
%semantic segmentation.

Study of the overhead imagery has been long started.
Efforts have been made to automate overhead image analysis. As early
as 1970~\cite{idelsohn1970learning} methods were introduced for
classifying terrain types from a single overhead image, with the goal
of automatically generating terrain maps.  Similarly, in 1976 Bajcsy
et al.~\cite{bajcsy1976computer} described a system for recognizing
roads, intersections, and other road-like objects in overhead imagery.
However, as overhead imaging differs drastically from ground-level
imaging, the majority of techniques that have been developed have
occurred independently and in task-specific ways~\cite{Rozen}.
Benefited from the fast growing monitoring satellite market, overhead
imagery grows fast nowadays. Microsoft BingMap provides public access
to their overhead image database with various resolution options.
Given the geographic information of the ground image, we are able to
obtain the corresponding overhead images to form the geo-orientation
aligned cross-view image pairs.
%
Given the fact that overhead imagery has much larger earth coverage
and better registered in geographic locations, many geo-localization
approaches~\cite{lin2013cross,lin2015learning,workman2015geocnn,workman2015wide}
extract the overhead features as reference database for
geo-localization.

%However, recognizing the geo-location and geo-orientation of an
%arbitrary outdoor image is an extremely challenging task.  Many
%methods have been proposed; the most common approach is to build a
%large database of images with known location and localize a query
%image using either local~\cite{li2010location,schindler2008detecting}
%or global~\cite{hays2008im2gps,doersch2012what} image features. 
%This approach is not applicable when no nearby ground-level imagery
%exists in the reference database, such as when the image was not
%captured near a popular tourist destination.  Even when reference
%imagery is available, the appearance of the objects may not be
%visually distinctive, for example a train track or a body of water. 


